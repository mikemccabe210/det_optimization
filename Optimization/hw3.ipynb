{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "#### (a) The function f is discontinuous and the set X is compact.  \n",
    "   has optimal solution:\n",
    " $ min\\ \\begin{cases} \n",
    "      |x| & x\\leq 1 \\\\\n",
    "      x + 1 & x\\gt 1 \\\\\n",
    "   \\end{cases}\\\\\n",
    "   s.t.\\ -5\\leq\\ x \\leq 5$  \n",
    "Our space is compact because it is a finite, closed set. Our function is discontinuous at x=1, however, the function is defined at it's infimum, meaning that we have a global minimum.   \n",
    "  \n",
    " no optimal solution:\n",
    " $ min\\ \\begin{cases} \n",
    "      -x & x\\lt 0 \\\\\n",
    "      x+1  & x\\geq 0 \\\\\n",
    "   \\end{cases}\\\\\n",
    "   s.t.\\ -5\\leq\\ x \\leq 5$   \n",
    "This example is mostly the same as our previous example. However, in this example, our discontinuity occurs at our infimum. Since our function jumps at 0, we could potentially choose closer at closer points to the jump and find lower values. It is impossible for us to reach a global minimum. For practical purposes, we would end up choosing a point very close to what a minimum would be if we switched which inequality we included, but never actually obtain a minimum.  \n",
    "  \n",
    "#### (b) The function f is continuous and the set X is not closed.  \n",
    "   has optimal solution:\n",
    " $ min\\  |x| \\\\\n",
    "   s.t.\\ -5\\lt\\ x \\lt 5$  \n",
    "In this case, our function has no discontinuity, but its limit points are not contained in the set. However, that does not impact the solution as our unique global minimum is not on a boundary.  \n",
    "  \n",
    " no optimal solution:\n",
    " $ min\\  5 - x \\\\\n",
    "   s.t.\\ -5\\lt\\ x \\lt 5$  \n",
    "Like in the second part of (a), our function becomes increasingly small as it approaches the edge, so that it can become smaller and smaller as we pick a more and more precise point. However, it continues to shrink as we reach infinite precision. Therefore, there is no global minimum.  \n",
    "#### (c) The function f is convex and the set X is not bounded.  \n",
    "   has optimal solution:\n",
    " $ min\\  |x| \\\\\n",
    "   s.t.\\ x\\in\\mathbb{R}$  \n",
    "If we remove the bounds from (b1), the example still holds. Our global minimum is still at 0. Since our function is increasing to infinity as x->infinity/-infinity, the lack of bounds do not matter.  \n",
    "  \n",
    " no optimal solution:\n",
    " $ min\\  5 - x \\\\\n",
    "   s.t.\\ x\\in\\mathbb{R}$  \n",
    "Now that our function is decreasing as x->infinity, we can never obtain an optimal solution because we could always get a lower value by moving towards infinity. \n",
    "  \n",
    "#### (d) The function f is convex and the set X is compact.  \n",
    "   has optimal solution:\n",
    " $ min\\  |x| \\\\\n",
    "   s.t.\\ -5\\leq\\ x \\leq 5$  \n",
    "As long as our domain includes x=0, our function will have an optimal solution as x=0.  \n",
    "  \n",
    " no optimal solution:\n",
    " $ DNE$  \n",
    "  \n",
    "   A convex function over a compact space will always have an optimal solution. \n",
    "   Our function is convex so our hessian/second derivative will always be PSD/non-negative. \n",
    "   This implies that our gradient/first derivative is either constant or increasing. \n",
    "   In the increasing case, if the gradient is negative on the lower bound, then it will either continue decreasing or hit a critical point and begin increasing. Since the hessian is PSD, we know the critical point is a local minimum. Since all local minima in convex functions are global minima, the local minimum would need to be an optimal solution. If there is no critical point, the function must be increasing/decreasing/constant across the domain, meaning that it must be lowest at an extreme (or at everyy point in the constant case). \n",
    "  \n",
    "In the constant case, then the function is either constantly increasing or decreasing, and a border must be a an optimal solution. Since our set is compact, our border is defined and the border must contain an optimal solution. \n",
    "\n",
    "#### (e) The function f is linear and the set X is not closed.  \n",
    "   has optimal solution:\n",
    " $ min\\ 2 \\\\\n",
    " s.t.\\ -5\\lt\\ x \\lt5 $  \n",
    "The converse of (d2). Since all functions with a 0 second derivative (this includes linear functions) must be increasing/decreasing/constant at a constant rate, non-constant functions must have an extrema at the border. Since the extreme points are not included in our set, the only optimal solution occurs for constant functions - in which case every point is a solution.  \n",
    "  \n",
    "no optimal solution:\n",
    " $min\\ x \\\\\n",
    " s.t.-5\\lt\\ x \\lt5$  \n",
    " \n",
    "Same explanation as (b2)\n",
    "  \n",
    "#### (f) The function f is linear and the set X is compact.  \n",
    "   has optimal solution:\n",
    " $ min\\  x \\\\\n",
    "   s.t.\\ -5\\leq\\ x \\leq 5$  \n",
    "Linear functions are convex, so any bounded linear function would work here.  \n",
    "\n",
    "no optimal solution:\n",
    " $ DNE$   \n",
    " \n",
    "Since all linear functions are convex, the same argument as in (d2) holds here.  \n",
    "\n",
    "#### End P1\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "#### (a) An optimization problem with a discontinuous objective function and a closed and bounded feasible region can never have an optimal solution.\n",
    "\n",
    "False. As long as the lowest left or right hand limiting value is equal to the function at the point that it occurs, there is an optimal solution.  \n",
    "  \n",
    "#### (b) Consider the optimization problem min f(x) s.t. g(x) ≤ 0.  Suppose the current optimal objective value is v. Now, if I change the right-hand-side of the constraint to 1 and resolve the problem, the new optimal objective value will be less than or equal to v.\n",
    "\n",
    "False. This would be an expansion of the feasible set. The range of f(x) over the original domain would be constant, meaning that if the range results in a new optimum, it must be lower than the current optimum. However, it is also possible that f is discontinuous in the expanded range and this results in there being no optimal solution.\n",
    "\n",
    "#### (c) Consider an optimization problem (P) : min f(x) s.t. x ∈ X where X is a non-empty closed convex set. Suppose that the problem (P) has the property that every local optimal solution is also globally optimal then f(x) must be a convex function.\n",
    "\n",
    "False. If the function is monotone, regardless of convexity, it will have a solution on an extreme point (assuming the set is closed and the extreme points are defined). min log(x) s.t. $x\\geq1$ is an example. \n",
    "\n",
    "#### (d) The problem (P): min x + y subject to x^2 + y^2 ≤ 4 is a convex optimization problem\n",
    "\n",
    "True. Our function is linear with respect to our arguments. All linear functions are convex. Our feasible set is the Euclidean ball of radius 2 around the origin, which is a convex set. Since our function and feasible set are both convex, it is a convex optimization problem.\n",
    "\n",
    "#### (e) If I solve an optimization problem, then add a new constraint to it and solve it again, the solution must change.\n",
    "\n",
    "False. New constraints do not necessesarily remove the portion of the feasible set that contains the optimal solution. For instance, any constraints added to x^2 that do not remove the critical point at x=0 from the domain will not change the solution. \n",
    "\n",
    "#### (f) Consider the following optimization problem: min[f(x)]^2 s.t. x ∈ X where f(x) is a nonconvex function and X is a non-empty set. Suppose at a feasible solution x∗ ∈ X the objective value is 0, then x∗must be an optimal solution.\n",
    "\n",
    "I'm reading the square as applying to the minimized term. In that case, false. The minimized value could potentially be negative, in which case its square would be positive and 0 would not necessarily be optimal. However, if the square were inside the min, then it would be optimal as the square would mean no negative solutions were possible. \n",
    "\n",
    "#### (g) If I maximize a univariate convex function over a closed interval then there has to be an optimal solution which is one of the end points.\n",
    "\n",
    "True. Since the hessian must be PSD, the gradient must be elementwise increasing or constant at each point. This means that if it is not a constant function then is a maximum of one critical point and it is a minimum by the second derivative test. If there is no critical point, then the function must be either non-decreasing or non-increasing. In either of these cases, the optimal solution would occur on an end point (or everywhere, including the ends, in the case of a constant function). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
